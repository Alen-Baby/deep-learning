{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "806d3a8c",
   "metadata": {},
   "source": [
    "# importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d5c194",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080b3de2",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed63a33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_json(\"/Users/alenbaby/Downloads/Amazon_Fashion.jsonl\" , lines=True) # Load dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a0e3ba",
   "metadata": {},
   "source": [
    "# Dataset info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f3a1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of dataset:\", df.shape)\n",
    "print(\"\\nColumn Info:\")\n",
    "print(df.info())\n",
    "print(\"\\nMissing values:\")\n",
    "print(df.isnull().sum())\n",
    "print(\"\\nSample data:\")\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3c8e66",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595ecab1",
   "metadata": {},
   "source": [
    "### Remove rows with missing text , remove duplicates , Keep only relevant columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ae9b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[[ \"text\", \"rating\"]] # Keep only relevant columns\n",
    "df = df.dropna(subset=[\"text\"]) # Remove rows with missing text\n",
    "df = df.drop_duplicates(subset=[\"text\", \"rating\"]) # Remove duplicate reviews\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3b7bcf",
   "metadata": {},
   "source": [
    "### remove conflict reviews using vader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f17fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "\n",
    "nltk.download(\"vader_lexicon\")\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "def conflict_filter_vader(text, rating):\n",
    "    score = sia.polarity_scores(text)[\"compound\"]\n",
    "\n",
    "    # compound score range: -1 (very negative) to +1 (very positive)\n",
    "    if rating >= 4 and score < -0.2:\n",
    "        return False\n",
    "    if rating <= 2 and score > 0.2:\n",
    "        return False\n",
    "\n",
    "    return True\n",
    "\n",
    "df = df[df.apply(lambda x: conflict_filter_vader(x[\"text\"], x[\"rating\"]), axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8f59c0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b14a9700",
   "metadata": {},
   "source": [
    "### new Column (Review_Length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bac571d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Review_Length\"] = df[\"text\"].apply(len) # Length of each review\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1423146e",
   "metadata": {},
   "source": [
    "### Remove short and long reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb68b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[\n",
    "    (df[\"Review_Length\"] >= 5) & # Minimum length filter\n",
    "    (df[\"Review_Length\"] <= 300) # Maximum length filter\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6e1d0f",
   "metadata": {},
   "source": [
    "# Display first few rows of cleaned dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68346e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head() # Display first few rows of cleaned dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37020278",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_lower(text):\n",
    "  \n",
    "    return text.lower() if isinstance(text, str) else text\n",
    "df[\"text\"] = df[\"text\"].apply(to_lower)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a151a193",
   "metadata": {},
   "source": [
    "### Expand contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326a4c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import contractions\n",
    "import pandas as pd\n",
    "\n",
    "def expand_contractions(text):\n",
    "    if pd.isna(text):\n",
    "        return text\n",
    "    return contractions.fix(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c834da16",
   "metadata": {},
   "source": [
    "### Normalisation and Cleaning (reuse code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3e58da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \n",
    "    text = re.sub(r\"http\\S+|www\\S+\", \"\", text)          # URLs\n",
    "    text = re.sub(r\"<.*?>\", \"\", text)                   # HTML\n",
    "    text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text)             # Emojis, punctuation\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6cded5",
   "metadata": {},
   "source": [
    "# Balanced Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61fa1607",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLES_PER_CLASS = 20000 # Balance classes by sampling\n",
    "\n",
    "df_balanced = (\n",
    "    df.groupby(\"rating\", group_keys=False)\n",
    "      .apply(lambda x: x.sample(\n",
    "          n=min(len(x), SAMPLES_PER_CLASS),\n",
    "          random_state=42\n",
    "      ))\n",
    ")\n",
    "\n",
    "print(df_balanced[\"rating\"].value_counts()) # Verify balanced classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53f6017",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_balanced.to_csv(\"balanced_reviews.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2019179b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display 3â€“10 full sample reviews per class (balanced)\n",
    "\n",
    "for cls in sorted(df[\"rating\"].unique()):\n",
    "    samples = df[df[\"rating\"] == cls].sample(n=3, random_state=42)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"Rating: {cls}\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    for i, review in enumerate(samples[\"text\"], 1):\n",
    "        print(f\"\\nReview {i}:\\n{review}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df27cdb5",
   "metadata": {},
   "source": [
    "### Rating Distribution - barchart "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26a900e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x=\"rating\", data=df_balanced)\n",
    "plt.title(\"Rating Distribution\")\n",
    "plt.xlabel(\"Rating\")\n",
    "plt.ylabel(\"Number of Reviews\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4fdfc96",
   "metadata": {},
   "source": [
    "### Review Lenth Distribution - histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa85183b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(x=\"Review_Length\", data=df_balanced)\n",
    "plt.title(\"Review Lenth Distribution\")\n",
    "plt.xlabel(\"Review lenth\")\n",
    "plt.ylabel(\"frequency\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3325892",
   "metadata": {},
   "source": [
    "### Review Length per Rating - boxplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63f66f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(y=\"Review_Length\",x=\"rating\", data=df_balanced)\n",
    "plt.title(\"Review Length per Rating\")\n",
    "plt.xlabel(\"Rating\")\n",
    "plt.ylabel(\"Review Length\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f6f7c4",
   "metadata": {},
   "source": [
    "### Violin Plot of Review Length per Rating - Violin plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b3ff51",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.violinplot(x=\"rating\", y=\"Review_Length\", data=df_balanced)\n",
    "plt.title(\"Violin Plot of Review Length per Rating\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379abc52",
   "metadata": {},
   "source": [
    "### stratified train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e738eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split # Split data into training and test sets\n",
    "\n",
    "X = df_balanced[\"text\"] # Use original text for modeling\n",
    "y = df_balanced[\"rating\"] # Target variable\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.2,stratify=y,shuffle=True,random_state=42) # Split data into training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a38570b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train.value_counts(normalize=True)) # Check class distribution in training set\n",
    "print(y_test.value_counts(normalize=True)) # Check class distribution in test set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb22291",
   "metadata": {},
   "source": [
    "### Normalisation and Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8793b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_clean = X_train.apply(to_lower) # Clean training data\n",
    "X_test_clean = X_test.apply(to_lower) \n",
    "X_train_clean = X_train_clean.apply(expand_contractions) # Clean training data\n",
    "X_test_clean = X_test_clean.apply(expand_contractions) \n",
    "X_train_clean = X_train_clean.apply(clean_text) # Clean training data\n",
    "X_test_clean = X_test_clean.apply(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f5625c",
   "metadata": {},
   "source": [
    "###  tokenize + pad function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c1cf7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "#  Fit tokenizer on TRAIN data only\n",
    "tokenizer = Tokenizer(oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(X_train_clean)\n",
    "\n",
    "#  Reusable tokenize + pad function\n",
    "def tokenize_and_pad(texts, tokenizer, max_len):\n",
    "    \"\"\"\n",
    "    Tokenizes and pads text using an existing fitted tokenizer.\n",
    "    \"\"\"\n",
    "    sequences = tokenizer.texts_to_sequences(texts)\n",
    "    return pad_sequences(\n",
    "        sequences,\n",
    "        maxlen=max_len,\n",
    "        padding=\"post\",\n",
    "        truncating=\"post\"\n",
    "    )\n",
    "\n",
    "#  Apply to TRAIN and TEST\n",
    "MAX_LEN = 200\n",
    "\n",
    "X_train_pad = tokenize_and_pad(X_train_clean, tokenizer, MAX_LEN)\n",
    "X_test_pad  = tokenize_and_pad(X_test_clean, tokenizer, MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b7c527",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(tokenizer.word_index) + 1\n",
    "print(\"Vocabulary size:\", VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbfc4e59",
   "metadata": {},
   "source": [
    "### Load GloVe embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1324865",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def load_glove_embeddings(glove_path):\n",
    "    embeddings_index = {}\n",
    "    with open(glove_path, encoding=\"utf8\") as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], dtype=\"float32\")\n",
    "            embeddings_index[word] = vector\n",
    "    return embeddings_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570c9f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_path = \"/Users/alenbaby/Downloads/glove.6B.100d.txt\"\n",
    "embeddings_index = load_glove_embeddings(glove_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf3c52b",
   "metadata": {},
   "source": [
    "### Create embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ffff81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedding_matrix(tokenizer, embeddings_index, embedding_dim):\n",
    "    vocab_size = len(tokenizer.word_index) + 1\n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "    for word, i in tokenizer.word_index.items():\n",
    "        if i < vocab_size:\n",
    "            vector = embeddings_index.get(word)\n",
    "            if vector is not None:\n",
    "                embedding_matrix[i] = vector\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4db34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 100\n",
    "embedding_matrix = create_embedding_matrix(\n",
    "    tokenizer,\n",
    "    embeddings_index,\n",
    "    EMBEDDING_DIM\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a52b69",
   "metadata": {},
   "source": [
    "### Use GloVe in the Embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11c2c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding\n",
    "\n",
    "embedding_layer = Embedding(\n",
    "    input_dim=embedding_matrix.shape[0],\n",
    "    output_dim=embedding_matrix.shape[1],\n",
    "    weights=[embedding_matrix],\n",
    "    input_length=MAX_LEN,\n",
    "    trainable=False   # keep pretrained semantics\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ae0ad5",
   "metadata": {},
   "source": [
    "# `."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937f5e08",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5791849",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, GlobalMaxPooling1D, Dense, Dropout\n",
    "\n",
    "def build_cnn_model():\n",
    "    model = Sequential([\n",
    "        embedding_layer,                     # trainable=False\n",
    "        Conv1D(filters=64, kernel_size=5, activation=\"relu\"),\n",
    "        GlobalMaxPooling1D(),\n",
    "        Dropout(0.5),\n",
    "        Dense(64, activation=\"relu\"),\n",
    "        Dense(5, activation=\"softmax\")\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=\"adam\",\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac8badd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert labels from 1â€“5 â†’ 0â€“4\n",
    "y_train_enc = y_train - 1\n",
    "y_test_enc  = y_test - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0cd9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stop = EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=2,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "cnn_model = build_cnn_model()\n",
    "cnn_model.summary()\n",
    "\n",
    "cnn_model.fit(\n",
    "    X_train_pad,\n",
    "    y_train_enc,\n",
    "    validation_data=(X_test_pad, y_test_enc),\n",
    "    epochs=10,\n",
    "    batch_size=64,\n",
    "    callbacks=[early_stop]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16053774",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras_tuner as kt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59143205",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn_tuned_model(hp):\n",
    "    model = Sequential()\n",
    "\n",
    "    # ðŸ”¹ Tunable embedding dimension\n",
    "    embedding_dim = hp.Choice(\"embedding_dim\", values=[50, 100, 200])\n",
    "\n",
    "    model.add(\n",
    "        Embedding(\n",
    "            input_dim=embedding_matrix.shape[0],\n",
    "            output_dim=embedding_dim,\n",
    "            weights=[embedding_matrix[:, :embedding_dim]],\n",
    "            input_length=MAX_LEN,\n",
    "            trainable=False\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # ðŸ”¹ CNN layer\n",
    "    model.add(Conv1D(filters=64, kernel_size=5, activation=\"relu\"))\n",
    "    model.add(GlobalMaxPooling1D())\n",
    "\n",
    "    # ðŸ”¹ Tunable dropout\n",
    "    dropout_rate = hp.Float(\"dropout\", min_value=0.3, max_value=0.6, step=0.1)\n",
    "    model.add(Dropout(dropout_rate))\n",
    "\n",
    "    model.add(Dense(64, activation=\"relu\"))\n",
    "    model.add(Dense(5, activation=\"softmax\"))\n",
    "\n",
    "    # ðŸ”¹ Tunable learning rate\n",
    "    learning_rate = hp.Choice(\"learning_rate\", values=[1e-2, 1e-3, 1e-4])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=learning_rate),\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7b6aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = kt.RandomSearch(\n",
    "    build_cnn_tuned_model,\n",
    "    objective=\"val_accuracy\",\n",
    "    max_trials=10,          # number of configurations tested\n",
    "    executions_per_trial=1,\n",
    "    directory=\"tuning\",\n",
    "    project_name=\"cnn_glove_tuning\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1c179f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.search(\n",
    "    X_train_pad,\n",
    "    y_train_enc,\n",
    "    validation_data=(X_test_pad, y_test_enc),\n",
    "    epochs=5,\n",
    "    batch_size=64\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31da834",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = tuner.get_best_models(num_models=1)[0]\n",
    "best_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7625b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = best_model.fit(\n",
    "    X_train_pad,\n",
    "    y_train_enc,\n",
    "    validation_data=(X_test_pad, y_test_enc),\n",
    "    epochs=10,\n",
    "    batch_size=64\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd36c4d",
   "metadata": {},
   "source": [
    "##  Bi-Directional LSTM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a4a576",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import LSTM, Bidirectional\n",
    "\n",
    "def build_bilstm_model():\n",
    "    model = Sequential([\n",
    "        embedding_layer,\n",
    "        Bidirectional(LSTM(128)),\n",
    "        Dropout(0.5),\n",
    "        Dense(64, activation=\"relu\"),\n",
    "        Dense(5, activation=\"softmax\")\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=\"adam\",\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3e1a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "bilstm_model = build_bilstm_model()\n",
    "bilstm_model.summary()\n",
    "\n",
    "bilstm_model.fit(\n",
    "    X_train_pad,\n",
    "    y_train_enc,\n",
    "    validation_data=(X_test_pad, y_test_enc),\n",
    "    epochs=10,\n",
    "    batch_size=64\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e923dea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import keras_tuner as kt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b6eee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_VOCAB_SIZE = len(tokenizer.word_index) + 1\n",
    "MAX_LEN = 200   # MUST match padding length\n",
    "\n",
    "def build_bilstm_model(hp):\n",
    "    model = Sequential()\n",
    "\n",
    "    # Embedding Layer\n",
    "    model.add(\n",
    "        Embedding(\n",
    "            input_dim=MAX_VOCAB_SIZE,\n",
    "            output_dim=hp.Choice(\"embedding_dim\", [64, 128, 256]),\n",
    "            input_length=MAX_LEN\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Bidirectional LSTM\n",
    "    model.add(\n",
    "        Bidirectional(\n",
    "            LSTM(\n",
    "                units=hp.Choice(\"lstm_units\", [64, 128]),\n",
    "                return_sequences=False\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Dropout\n",
    "    model.add(\n",
    "        Dropout(\n",
    "            hp.Float(\"dropout\", min_value=0.2, max_value=0.5, step=0.1)\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Output Layer (5-class)\n",
    "    model.add(Dense(5, activation=\"softmax\"))\n",
    "\n",
    "    # Compile INSIDE the function\n",
    "    model.compile(\n",
    "        optimizer=Adam(\n",
    "            learning_rate=hp.Choice(\"learning_rate\", [1e-4, 3e-4, 1e-3])\n",
    "        ),\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d6106a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = kt.RandomSearch(\n",
    "    build_bilstm_model,\n",
    "    objective=\"val_accuracy\",\n",
    "    max_trials=10,\n",
    "    directory=\"bilstm_tuning\",\n",
    "    project_name=\"pretokenized_input\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef94cb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.search(\n",
    "    X_train_pad,\n",
    "    y_train_enc,\n",
    "    validation_split=0.2,\n",
    "    epochs=5,\n",
    "    batch_size=128,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631a99a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = tuner.get_best_models(num_models=1)[0]\n",
    "\n",
    "history = best_model.fit(\n",
    "    X_train_pad,\n",
    "    y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=10,\n",
    "    batch_size=128\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818cccf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = best_model.evaluate(X_test_pad, y_test)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d891d342",
   "metadata": {},
   "source": [
    "##  Bi-Directional GRU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0ff234",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import GRU\n",
    "\n",
    "def build_bigru_model():\n",
    "    model = Sequential([\n",
    "        embedding_layer,\n",
    "        Bidirectional(GRU(128)),\n",
    "        Dropout(0.5),\n",
    "        Dense(64, activation=\"relu\"),\n",
    "        Dense(5, activation=\"softmax\")\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=\"adam\",\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7c66ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigru_model = build_bigru_model()\n",
    "bigru_model.summary()\n",
    "\n",
    "bigru_model.fit(\n",
    "    X_train_pad,\n",
    "    y_train_enc,\n",
    "    validation_data=(X_test_pad, y_test_enc),\n",
    "    epochs=10,\n",
    "    batch_size=64\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1b2179",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_acc    = cnn_model.evaluate(X_test_pad, y_test_enc)[1]\n",
    "bilstm_acc = bilstm_model.evaluate(X_test_pad, y_test_enc)[1]\n",
    "bigru_acc  = bigru_model.evaluate(X_test_pad, y_test_enc)[1]\n",
    "\n",
    "print(\"CNN Accuracy:\", cnn_acc)\n",
    "print(\"BiLSTM Accuracy:\", bilstm_acc)\n",
    "print(\"BiGRU Accuracy:\", bigru_acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
